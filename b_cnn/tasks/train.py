import keras
import numpy as np
import os
from keras.datasets import cifar10
from keras.models import Model
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D, Input
from keras import optimizers
from keras.callbacks import LearningRateScheduler, TensorBoard
from keras.layers.normalization import BatchNormalization
from keras import backend as K

def scheduler(epoch):
  learning_rate_init = 0.003
  if epoch > 40:
    learning_rate_init = 0.0005
  if epoch > 50:
    learning_rate_init = 0.0001
  return learning_rate_init

class LossWeightsModifier(keras.callbacks.Callback):
  def __init__(self, alpha, beta, gamma):
    self.alpha = alpha
    self.beta = beta
    self.gamma = gamma
  def on_epoch_end(self, epoch, logs={}):
    if epoch == 8:
      K.set_value(self.alpha, 0.1)
      K.set_value(self.beta, 0.8)
      K.set_value(self.gamma, 0.1)
    if epoch == 18:
      K.set_value(self.alpha, 0.1)
      K.set_value(self.beta, 0.2)
      K.set_value(self.gamma, 0.7)
    if epoch == 28:
      K.set_value(self.alpha, 0)
      K.set_value(self.beta, 0)
      K.set_value(self.gamma, 1)


def train():


    #-------- dimensions ---------
    img_rows, img_cols = 32, 32
    if K.image_data_format() == 'channels_first':
        input_shape = (3, img_rows, img_cols)
    else:
        input_shape = (img_rows, img_cols, 3)
    #-----------------------------

    train_size = 50000

    #--- coarse 1 classes ---
    num_c_1 = 2
    #--- coarse 2 classes ---
    num_c_2 = 7
    #--- fine classes ---
    num_classes  = 10

    batch_size   = 128
    # epochs       = 60
    epochs       = 1


    #-------------------- data loading ----------------------
    (x_train, y_train), (x_test, y_test) = cifar10.load_data()
    y_train = keras.utils.to_categorical(y_train, num_classes)
    y_test = keras.utils.to_categorical(y_test, num_classes)
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')

    #---------------- data preprocessiong -------------------
    x_train = (x_train-np.mean(x_train)) / np.std(x_train)
    x_test = (x_test-np.mean(x_test)) / np.std(x_test)

    #---------------------- make coarse 2 labels --------------------------
    parent_f = {
      2:3, 3:5, 5:5,
      1:2, 7:6, 4:6,
      0:0, 6:4, 8:1, 9:2
    }
    y_c2_train = np.zeros((y_train.shape[0], num_c_2)).astype("float32")
    y_c2_test = np.zeros((y_test.shape[0], num_c_2)).astype("float32")
    for i in range(y_c2_train.shape[0]):
      y_c2_train[i][parent_f[np.argmax(y_train[i])]] = 1.0
    for i in range(y_c2_test.shape[0]):
      y_c2_test[i][parent_f[np.argmax(y_test[i])]] = 1.0

    #---------------------- make coarse 1 labels --------------------------
    parent_c2 = {
      0:0, 1:0, 2:0,
      3:1, 4:1, 5:1, 6:1
    }
    y_c1_train = np.zeros((y_c2_train.shape[0], num_c_1)).astype("float32")
    y_c1_test = np.zeros((y_c2_test.shape[0], num_c_1)).astype("float32")
    for i in range(y_c1_train.shape[0]):
      y_c1_train[i][parent_c2[np.argmax(y_c2_train[i])]] = 1.0
    for i in range(y_c1_test.shape[0]):
      y_c1_test[i][parent_c2[np.argmax(y_c2_test[i])]] = 1.0


    #----------------------- model definition ---------------------------
    alpha = K.variable(value=0.98, dtype="float32", name="alpha") # A1 in paper
    beta = K.variable(value=0.01, dtype="float32", name="beta") # A2 in paper
    gamma = K.variable(value=0.01, dtype="float32", name="gamma") # A3 in paper

    img_input = Input(shape=input_shape, name='input')

    #--- block 1 ---
    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)
    x = BatchNormalization()(x)
    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)

    #--- block 2 ---
    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)
    x = BatchNormalization()(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)

    #--- coarse 1 branch ---
    c_1_bch = Flatten(name='c1_flatten')(x)
    c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)
    c_1_bch = BatchNormalization()(c_1_bch)
    c_1_bch = Dropout(0.5)(c_1_bch)
    c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)
    c_1_bch = BatchNormalization()(c_1_bch)
    c_1_bch = Dropout(0.5)(c_1_bch)
    c_1_pred = Dense(num_c_1, activation='softmax', name='c1_predictions_cifar10')(c_1_bch)

    #--- block 3 ---
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)
    x = BatchNormalization()(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)

    #--- coarse 2 branch ---
    c_2_bch = Flatten(name='c2_flatten')(x)
    c_2_bch = Dense(512, activation='relu', name='c2_fc_cifar10_1')(c_2_bch)
    c_2_bch = BatchNormalization()(c_2_bch)
    c_2_bch = Dropout(0.5)(c_2_bch)
    c_2_bch = Dense(512, activation='relu', name='c2_fc2')(c_2_bch)
    c_2_bch = BatchNormalization()(c_2_bch)
    c_2_bch = Dropout(0.5)(c_2_bch)
    c_2_pred = Dense(num_c_2, activation='softmax', name='c2_predictions_cifar10')(c_2_bch)

    #--- block 4 ---
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)
    x = BatchNormalization()(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)

    #--- fine block ---
    x = Flatten(name='flatten')(x)
    x = Dense(1024, activation='relu', name='fc_cifar10_1')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(1024, activation='relu', name='fc2')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    fine_pred = Dense(num_classes, activation='softmax', name='predictions_cifar10')(x)

    model = Model(input=img_input, output=[c_1_pred, c_2_pred, fine_pred], name='medium_dynamic')

    #----------------------- compile and fit ---------------------------
    sgd = optimizers.SGD(lr=0.003, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy',
                  optimizer=sgd,
                  loss_weights=[alpha, beta, gamma],
                  # optimizer=keras.optimizers.Adadelta(),
                  metrics=['accuracy'])

    tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)
    change_lr = LearningRateScheduler(scheduler)
    change_lw = LossWeightsModifier(alpha, beta, gamma)
    cbks = [change_lr, tb_cb, change_lw]

    model.fit(x_train, [y_c1_train, y_c2_train, y_train],
              batch_size=batch_size,
              epochs=epochs,
              verbose=1,
              callbacks=cbks,
              validation_data=(
                  x_test,
                  [y_c1_test, y_c2_test, y_test]
              ))

    #---------------------------------------------------------------------------------
    # The following compile() is just a behavior to make sure this model can be saved.
    # We thought it may be a bug of Keras which cannot save a model compiled with loss_weights parameter
    #---------------------------------------------------------------------------------
    model.compile(loss='categorical_crossentropy',
                # optimizer=keras.optimizers.Adadelta(),
                optimizer=sgd,
                metrics=['accuracy'])

    score = model.evaluate(x_test, [y_c1_test, y_c2_test, y_test], verbose=0)
    model.save(model_path)
    print('score is: ', score)


if __name__ == "__main__":
    train()
